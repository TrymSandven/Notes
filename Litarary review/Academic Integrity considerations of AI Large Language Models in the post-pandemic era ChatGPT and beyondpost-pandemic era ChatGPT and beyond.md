
Academic Integrity considerations of AI Large Language Models in the post-pandemic era: ChatGPT and beyondpost-pandemic era: ChatGPT and beyond
========================
by Mike Perkins  

### Looks at the academic integrity considerations of students' use of AI in formal assessments. Further looks at how the use of said tools can avoid detection by existing technologies and staff. Concludes with that "Deciding whether any particular use of LLMs by students can be defined as academic misconduct is determined by the academic integrity policies of any given HEI, which must be updated to consider how these tools will be used in future educational environments"

## Literature
Looks at the history of writing aids, such as Digital Writing Assistants (DWAs), Automated Paraphrasing Tools (APTs) and lastly LLMs. Demonstrates that ChatGPT can write very "fluently" as well as "cite" sources, however they are entirely fabricated. Further demonstrates that ChatGPT can be factually wrong in some cases but will answer as if it is true. 

## Academic integrity and considerations of LLMs
Academic staff may be unable to identify the amount of work produced by a student. Claims there is not a 100% agreement if the use of LLMs can be considered a breach of academic integrity or not. Coping text directly from the output and not stating the use of AI should be considered a clear case of plagiarism. "If any output text has been modified, edited or enhanced, or if output text has been used as a starting point for the formation of an argument. If additional scholarly work has been carried out by the student, such as the integration of the output text with the use of examples and cited sources, then this complicates the situation further." This does not fall under the strict definition of plagiarism but could be considered academic misconduct depending on the HIEs policies.

## Can the use of LLMs be detected by academic staff?
Cites other studies were there has been done testing to see if academic staff can recognice LLMs. The result is that it they can not with an accuracy of 49,9&-57,9% (n=780)

## Technological methods of detection
Gehrmann et al. (2019) tested tecnological methods to identify GPT-2 output with an accuracy of 54%-74% compared to student written text. Fröhling & Zubiaga (2021) present a promising low cost detection model which is able to accurately detect machine created text created using GPT-2 and GPT-3, but highlight the ethical challenges of deploying any such detectors which may potentially discriminate against EFL students by incorrectly identifying human created text as machine written —a particular concern in HEIs with a high concentration of non-native English speaking students.

Organisations have either released, or have announced the imminent release of tools which claim to have the ability to
detect AI generated text. These tools include GPTZero (https://gptzero.me/) and Crossplag AI detect (https://crossplag.com/ai-content-detector/) and do show promise in being able to detect the use of AI generated text. However, further study is required to identify the accuracy of these tools

## Conclusion
The situation, coupled with the identified difficulties in determining whether we can even consider the use of such tools to be a breach of academic integrity, and the potential benefits of LLM based tools means that a blanket ban of these tools at an institutional level is neither feasible, nor enforceable. Despite the potential threats to academic integrity presented in the paper, we believe that the future integration of LLMs and other AI supported digital tools into the classroom environment is highly likely, and therefore HEIs must consider the implications of this in future policy development. 